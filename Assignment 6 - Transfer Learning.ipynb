{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1cc0a94988ba42a8bceb921194493196":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4629c056f6c9464c81317fc6c28d83c4","IPY_MODEL_c6cf578b532944b890e8233b06d8b3c4","IPY_MODEL_d80662d5671b4eb8a66027c359930c2b"],"layout":"IPY_MODEL_ebd32113f46e424398825d8bbd254c0c"}},"4629c056f6c9464c81317fc6c28d83c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a09f2e5b9d024414baea926378db4a1d","placeholder":"​","style":"IPY_MODEL_be232b45d69040d0a6fc5eaada230076","value":"Dl Completed...: 100%"}},"c6cf578b532944b890e8233b06d8b3c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a8dd115767246048c277de1d6fb0df6","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9486267664b41ab854ae86049d8bdae","value":5}},"d80662d5671b4eb8a66027c359930c2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e21beb1bf8949638c85521121c2d358","placeholder":"​","style":"IPY_MODEL_6d5a64d100684e80a3d11fad32297d74","value":" 5/5 [00:04&lt;00:00,  1.10 file/s]"}},"ebd32113f46e424398825d8bbd254c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a09f2e5b9d024414baea926378db4a1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be232b45d69040d0a6fc5eaada230076":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a8dd115767246048c277de1d6fb0df6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9486267664b41ab854ae86049d8bdae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e21beb1bf8949638c85521121c2d358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d5a64d100684e80a3d11fad32297d74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["**LP - 4 Assignment 6**"],"metadata":{"id":"z8d148nS2lUw"}},{"cell_type":"code","source":["# example of using a pre-trained model as a classifier\n","from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.applications.vgg16 import decode_predictions\n","from keras.applications.vgg16 import VGG16\n","# load an image from file\n","image = load_img('dog.jpg', target_size=(224, 224))\n","# convert the image pixels to a numpy array\n","image = img_to_array(image)\n","# reshape data for the model\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","# prepare the image for the VGG model\n","image = preprocess_input(image)\n","# load the model\n","model = VGG16()\n","# predict the probability across all output classes\n","yhat = model.predict(image)\n","# convert the probabilities to class labels\n","label = decode_predictions(yhat)\n","# retrieve the most likely result, e.g. highest probability\n","label = label[0][0]\n","# print the classification\n","print('%s (%.2f%%)' % (label[1], label[2]*100))"],"metadata":{"id":"a9ElT-TkZdZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"REL5gTVY26QF"}},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"fsLjd2vR27Mp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"sip5CbbM29VR"}},{"cell_type":"markdown","source":["Explore more about datset: https://www.tensorflow.org/datasets/catalog/tf_flowers"],"metadata":{"id":"D9KKPgUs2_E5"}},{"cell_type":"code","source":["## Loading images and labels\n","(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\"tf_flowers\",\n","    split=[\"train[:70%]\", \"train[:30%]\"], ## Train test split\n","    batch_size=-1,\n","    as_supervised=True,  # Include labels\n",")"],"metadata":{"id":"mGBkE7V73A6B","colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["1cc0a94988ba42a8bceb921194493196","4629c056f6c9464c81317fc6c28d83c4","c6cf578b532944b890e8233b06d8b3c4","d80662d5671b4eb8a66027c359930c2b","ebd32113f46e424398825d8bbd254c0c","a09f2e5b9d024414baea926378db4a1d","be232b45d69040d0a6fc5eaada230076","0a8dd115767246048c277de1d6fb0df6","f9486267664b41ab854ae86049d8bdae","9e21beb1bf8949638c85521121c2d358","6d5a64d100684e80a3d11fad32297d74"]},"outputId":"0e129ffb-78f4-4974-ef53-2c96db7d0017"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset 218.21 MiB (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to ~/tensorflow_datasets/tf_flowers/3.0.1...\n"]},{"output_type":"display_data","data":{"text/plain":["Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc0a94988ba42a8bceb921194493196"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset tf_flowers downloaded and prepared to ~/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n"]}]},{"cell_type":"markdown","source":["### Image Preprocessing"],"metadata":{"id":"pC2DKXrf3Cbd"}},{"cell_type":"code","source":["## check existing image size\n","train_ds[0].shape"],"metadata":{"id":"b7sT3lUn3EnX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb1318c3-996d-496b-8d80-24a0f994afe4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([150, 150, 3])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["## Resizing images\n","train_ds = tf.image.resize(train_ds, (150, 150))\n","test_ds = tf.image.resize(test_ds, (150, 150))"],"metadata":{"id":"EEsr6iG_3Gog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels"],"metadata":{"id":"gRjo0SO23Jtt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8523a545-6bca-462c-8ac5-ab7461ca5c6f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2569,), dtype=int64, numpy=array([2, 3, 3, ..., 0, 2, 0])>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["## Transforming labels to correct format\n","train_labels = to_categorical(train_labels, num_classes=5)\n","test_labels = to_categorical(test_labels, num_classes=5)"],"metadata":{"id":"kqwOCnMb3L--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels[0]"],"metadata":{"id":"8dYMfKn43MWC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d9c85988-5274-4aac-cbee-1e5363a5d504"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 1., 0., 0.], dtype=float32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# example of using a pre-trained model as a classifier\n","from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.applications.vgg16 import decode_predictions\n","from keras.applications.vgg16 import VGG16\n","# load an image from file\n","image = load_img('dog.jpg', target_size=(224, 224))\n","# convert the image pixels to a numpy array\n","image = img_to_array(image)\n","# reshape data for the model\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","# prepare the image for the VGG model\n","image = preprocess_input(image)\n","# load the model\n","model = VGG16()\n","# predict the probability across all output classes\n","yhat = model.predict(image)\n","# convert the probabilities to class labels\n","label = decode_predictions(yhat)\n","# retrieve the most likely result, e.g. highest probability\n","label = label[0][0]\n","# print the classification\n","print('%s (%.2f%%)' % (label[1], label[2]*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"7jcDM7eyYHzL","outputId":"232115f5-7c40-4484-e8ac-273547e23654"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5f4e6f5d1772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load an image from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dog.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# convert the image pixels to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m       \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dog.jpg'"]}]},{"cell_type":"markdown","source":["### Use Pretrained VGG16 Image Classification model"],"metadata":{"id":"s7HgCz0P3IEB"}},{"cell_type":"markdown","source":["# **Load a pre-trained CNN model trained on a large dataset**"],"metadata":{"id":"t6rTWsdy3PxO"}},{"cell_type":"code","source":["from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input"],"metadata":{"id":"UTNLDoDx3Rlw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds[0].shape"],"metadata":{"id":"Zu6ifxTt3Tax","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0c6ccb0-1935-47fa-d290-1451d53bd939"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([150, 150, 3])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["## Loading VGG16 model\n","base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)"],"metadata":{"id":"_foKs8a23Thw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"871a8ed7-1ef6-4b1b-9c3b-c513e41a1770"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["## will not train base mode\n","# Freeze Parameters in model's lower convolutional layers\n","base_model.trainable = False "],"metadata":{"id":"xn0SzAzT3gXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Preprocessing input\n","train_ds = preprocess_input(train_ds) \n","test_ds = preprocess_input(test_ds)"],"metadata":{"id":"KzSmxmXq3gZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## model details\n","base_model.summary()"],"metadata":{"id":"60JgLKcK3ge8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be720e58-948d-4625-ee9c-1d1da09a0ad6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["**Add custom classifier with two dense layers of trainable parameters to model**"],"metadata":{"id":"0iqFHkdD3ehF"}},{"cell_type":"code","source":["#add our layers on top of this model\n","from tensorflow.keras import layers, models\n","\n","flatten_layer = layers.Flatten()\n","dense_layer_1 = layers.Dense(50, activation='relu')\n","dense_layer_2 = layers.Dense(20, activation='relu')\n","prediction_layer = layers.Dense(5, activation='softmax')\n","\n","\n","model = models.Sequential([\n","    base_model,\n","    flatten_layer,\n","    dense_layer_1,\n","    dense_layer_2,\n","    prediction_layer\n","])"],"metadata":{"id":"EXI5bwPp3pAd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train classifier layers on training data available for task**"],"metadata":{"id":"pQ0ZvUuM3sye"}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy'],\n",")"],"metadata":{"id":"UPzA3chu3udW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)"],"metadata":{"id":"_z6UIdLV3uf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history=model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"],"metadata":{"id":"viWAwuJb3uiu","colab":{"base_uri":"https://localhost:8080/","height":328},"outputId":"60070d18-743a-45ee-c6ac-82f3f2c8d1fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-2223e6b9b643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       _, _, filtered_flat_args = (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["los,accurac=model.evaluate(test_ds,test_labels)\n","print(\"Loss: \",los,\"Accuracy: \", accurac)"],"metadata":{"id":"I0_GdLhz3um0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1dbe336-9b39-47d9-d171-040dff3c06fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["35/35 [==============================] - 227s 6s/step - loss: 5.9854 - accuracy: 0.2089\n","Loss:  5.985446453094482 Accuracy:  0.20890100300312042\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['accuracy'])\n","plt.title('ACCURACY')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train'],loc='upper left')\n","plt.show()"],"metadata":{"id":"wg47Hl6F310l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","y_pred = model.predict(test_ds)\n","y_classes = [np.argmax(element) for element in y_pred]\n","#to_categorical(y_classes, num_classes=5)\n","#to_categorical(test_labels, num_classes=5)\n","print(y_classes[:10])\n","print(\"\\nTest\")\n","print(test_labels[:10])"],"metadata":{"id":"7ddidxVC344b"},"execution_count":null,"outputs":[]}]}